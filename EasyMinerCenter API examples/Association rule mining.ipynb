{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Association Rule Mining using EasyMiner API\n",
    "\n",
    "This example demonstrates the possibility of association rule mining using complex REST API of data mining system EasyMiner.\n",
    "<br /><br />\n",
    "To use this example, you must have a working instance of EasyMiner. For testing purposes, you can use our demo server.\n",
    "\n",
    "## Dataset IRIS\n",
    "\n",
    "This example code is based on daset IRIS from the [UCI Repository](https://archive.ics.uci.edu/ml/datasets/iris). The file used in this exmample is located in the folder with this notebook: [iris.csv](./iris.csv)\n",
    "<br /><br />\n",
    "The dataset contains columns *sepallength*, *petalwidth*, *sepalwidth*, *petallength* and *class*. For rule miming also as for classification model building, the column *class* should be used in consequent part of rules, other columns should be used in antecedent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup variables, import dependencies\n",
    "\n",
    "To run this example, you have to configure the following variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import requested libraries\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import urllib\n",
    "\n",
    "# Setup details about the used file\n",
    "CSV_FILE = 'iris.csv'\n",
    "CSV_SEPARATOR = ','\n",
    "CSV_ENCODING = 'utf8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the integrated data mining API provided by EasyMiner, you must have an user account on a running instance of EasyMiner. Please input the URL of the API in the variable *API_URL*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure access variables\n",
    "API_URL = 'https://br-dev.lmcloud.vse.cz/easyminercenter/api'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with EasyMiner you must register an user account.\n",
    "It can be realized using the GUI also as using the API. \n",
    "<br />\n",
    "If you already have an account, please input your API KEY in the following variable *API_KEY*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you do not have an user account yet,\n",
    "you can register a new one using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if API_KEY == \"\":\n",
    "    user_name = 'testuser' + str(time.time())\n",
    "    user_email = user_name + \"@domain.tld\"\n",
    "    user_password = user_name\n",
    "    \n",
    "    # JSON configuration of the API request\n",
    "    json_data = json.dumps({\"name\": user_name, \"email\": user_email, \"password\": user_password})\n",
    "\n",
    "    # Send request for miner creation\n",
    "    r = requests.post(API_URL + \"/users?apiKey=\" + API_KEY, headers = {'Content-Type': 'application/json', \"Accept\": \"application/json\"}, data = json_data.encode())\n",
    "    \n",
    "    # Get the API key of the newly registered user account\n",
    "    API_KEY = r.json()[\"apiKey\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please check the configuration using the simple API call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the functionality of the user account\n",
    "r = requests.get(API_URL + \"/auth?apiKey=\" + API_KEY, headers={\"Accept\": \"application/json\"})\n",
    "\n",
    "# Parse the response as JSON\n",
    "auth_user = r.json()\n",
    "\n",
    "# If everything works correctly, you should get the details of your account:\n",
    "auth_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload CSV file to EasyMiner server (create datasource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP request for uploading of the CSV file\n",
    "r = requests.post(API_URL + '/datasources?separator=' + urllib.parse.quote(CSV_SEPARATOR) + '&encoding=' + CSV_ENCODING + '&type=limited&apiKey=' + API_KEY, files = {(\"file\", open(CSV_FILE, 'rb'))}, headers = {\"Accept\": \"application/json\"})\n",
    "\n",
    "# Get datasource ID (identificates the dataset on EasyMiner server) from the server response\n",
    "datasource_id = r.json()[\"id\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For debug purposes, print datasource_id - if the datasource was created successfully, the datasource_id should be greater than 0)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define name for the miner {optional value for your better orientation in list of miners]\n",
    "miner_name = 'TEST MINER'\n",
    "\n",
    "# JSON configuration of the API request (will be sent as body of the HTTP request)\n",
    "json_data = json.dumps({\"name\": miner_name, \"type\": \"cloud\", \"datasourceId\": datasource_id})\n",
    "\n",
    "# Send request for miner creation\n",
    "r = requests.post(API_URL + \"/miners?apiKey=\" + API_KEY, headers = {'Content-Type': 'application/json', \"Accept\": \"application/json\"}, data = json_data.encode())\n",
    "\n",
    "# Get ID of the created miner (identificates the miner on EasyMiner server)\n",
    "miner_id = r.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For debug purposes, print datasource_id - if the datasource was created successfully, the datasource_id should be greater than )  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocess data \n",
    "It is not possible to use the uploaded data fields from the uploaded datasource directly for definition of the data mining task. You have to generate attribute from each attribute you want to use.\n",
    "<br /><br />\n",
    "The simplest preprocessing method is to use the values of the data field \"as they are\" using the preprocessing method \"each value - one bin\".\n",
    "<br /><br />\n",
    "The uploaded data fields are identified using their names. Remember, the names has not be exactly the same as in the uploaded file (in case of duplicities etc.). You should get the list of data fields (columns) in the datasource:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request from the EasyMiner list of columns (data fields) available in the existing datasource\n",
    "r = requests.get(API_URL + '/datasources/' + str(datasource_id) + '?apiKey=' + API_KEY, headers = {'Content-Type': 'application/json', \"Accept\": \"application/json\"})\n",
    "\n",
    "# The response contains properties of the datasource also as the list of columns. Get only the columns... \n",
    "datasource_columns = r.json()['column']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the list of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of preprocessing requests - simple usage of the original data values \n",
    "\n",
    "In case you want to preprocess all the columns from the data field using the method \"each value - one bin\", you can simple use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variable for collecting of list of prepared attributes\n",
    "attributes_columns_map = {}\n",
    "\n",
    "# Process all the columns...\n",
    "for col in datasource_columns:\n",
    "    # You can work with the column name or the column ID. Both these values are parsed from the previous JSON response.\n",
    "    column_name = col['name']\n",
    "    \n",
    "    # You have to select \n",
    "    attribute_name = column_name\n",
    "    \n",
    "    # Construct the definition of preprocessing request; \n",
    "    # for identification of the column from datasource, you can use its ID (set it to property \"column\"), or its name (set it to property \"columnName\").. \n",
    "    json_data = json.dumps({\"miner\": miner_id, \"name\": attribute_name, \"columnName\": column_name, \"specialPreprocessing\": \"eachOne\"})\n",
    "    \n",
    "    # Send the request and wait for the response;\n",
    "    # dependently on the size of the used datasource, it can take a bit longer time...\n",
    "    r = requests.post(API_URL + \"/attributes?apiKey=\" + API_KEY, headers = {'Content-Type': 'application/json', \"Accept\": \"application/json\"}, data = json_data.encode())\n",
    "    if r.status_code != 201:\n",
    "        break  # error occurred - the preprocessing of the selected attribute failed\n",
    "    attributes_columns_map[column_name] = r.json()['name']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of prepared attributes is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_columns_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.b Other preprocessing methods\n",
    "\n",
    "New version of the data mining system EasyMiner supports also all\n",
    "standard preprocessing methods for preparation of attributes from data fields (datasource columns):\n",
    " - *equidistant intervals* - group numerical values to intervals with given length or to defined count of intervals\n",
    " - *equifrequent intervals* - group numerical values to given count of intervals with almost the same frequencies of values in the datasource\n",
    " - *equisized intervals* - group numerical values to intervals with requested minimal value of support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define association rule mining task\n",
    "\n",
    "Define attributes for the antecedent and consequent parts of association rules. The attributes can be configured  to either appear with any value or constrained to only one fixed value.\n",
    "<br /><br />\n",
    "This step also entails definition of threshold values on interest measures (confidence, support, lift) and optionally you can also enable CBA prunning of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pattern of association rules you are interested in\n",
    "antecedent = [\n",
    "    {\"attribute\" : attributes_columns_map[\"sepallength\"]},\n",
    "    {\"attribute\" : attributes_columns_map[\"petalwidth\"]},\n",
    "    {\"attribute\" : attributes_columns_map[\"sepalwidth\"]},\n",
    "    {\"attribute\" : attributes_columns_map[\"petallength\"]}\n",
    "]\n",
    "consequent = [\n",
    "    {\n",
    "        \"attribute\" : attributes_columns_map[\"class\"],\n",
    "        # Optionally, you can also select only one value of the given attribute \n",
    "        # - uncommenting and editing the following line.\n",
    "        # The same option works also for attributes in antecedent.\n",
    "        # \"fixedValue\" : \"Iris-setosa\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define requested interesting measures:\n",
    "# - following definition of requests minimal values of confidence 0.5 and support 0.1;\n",
    "# - with the same structure, you can add also the interest measure \"LIFT\"\n",
    "interest_measures = [\n",
    "    {\n",
    "        \"name\": \"CONF\", # considence\n",
    "        \"value\": 0.5\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"SUPP\", # support\n",
    "        \"value\": 0.1\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define the name of the prepared task (for better identification of results)\n",
    "task_name = \"Test task\"\n",
    "\n",
    "# Define the maximum count of results\n",
    "max_rules_count = 1000\n",
    "\n",
    "# Compose the body of the task definition request\n",
    "json_data = json.dumps({\n",
    "    \"miner\": miner_id,\n",
    "    \"name\": task_name,\n",
    "    \"limitHits\": max_rules_count,\n",
    "    \"IMs\": interest_measures,\n",
    "    \"antecedent\": antecedent,\n",
    "    \"consequent\": consequent\n",
    "})\n",
    "\n",
    "# Send the request for simple task creation\n",
    "r = requests.post(API_URL + \"/tasks/simple?apiKey=\" + API_KEY, headers = {'Content-Type': 'application/json', \"Accept\": \"application/json\"}, data = json_data.encode())\n",
    "\n",
    "# Get the ID of the created task\n",
    "task_id = r.json()[\"id\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task ID is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Execute the mining task\n",
    "\n",
    "Everything is prepared! You can execute the task and then work with the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the request\n",
    "r = requests.get(API_URL + \"/tasks/\" + str(task_id) + \"/start?apiKey=\" + API_KEY, headers = {'Content-Type': 'application/json', \"Accept\": \"application/json\"})\n",
    "\n",
    "# Wait for result (dependently on the task definition and the size of analyzed data, it can take even a long tame)\n",
    "while True:\n",
    "    time.sleep(1)\n",
    "    # Check the task state\n",
    "    r = requests.get(API_URL + \"/tasks/\" + str(task_id) + \"/state?apiKey=\" + API_KEY, headers = {'Content-Type': 'application/json', \"Accept\": \"application/json\"})\n",
    "    task_state = r.json()[\"state\"]\n",
    "    print(\"task_state:\" + task_state)\n",
    "    if task_state == \"solved\":\n",
    "        break\n",
    "    if task_state == \"failed\":\n",
    "        print(\"task failed executing\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Export the task results\n",
    "\n",
    "The task results can be exported in the following formats: \n",
    " - simple JSON format (suitable for simple reading of results etc.)\n",
    " - standardized PMML Association Model (we recommend the version 4.2)\n",
    " - GUHA PMML\n",
    "\n",
    "## 6.a Simple export of association rules as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the export request\n",
    "r = requests.get(API_URL + '/tasks/' + str(task_id) + '/rules?apiKey=' + API_KEY, headers = {\"Accept\": \"application/json\"})\n",
    "\n",
    "# Parse the response as JSON\n",
    "task_rules = r.json()\n",
    "\n",
    "# and then work with the results..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results in JSON are: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.b Export results as PMML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the PMML format - possible values are \"guha\", \"associationmodel\", \"associationmodel-4.2\"\n",
    "pmml_format = \"associationmodel\" \n",
    "\n",
    "# Send the PMML export request\n",
    "r = requests.get(API_URL + '/tasks/' + str(task_id) + '/pmml?model=' + pmml_format + '&apiKey=' + API_KEY)\n",
    "\n",
    "# Get the response as text (and then parse it as XML etc.)\n",
    "pmml = r.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Definition of another data mining task\n",
    "\n",
    "When you have already prepared the attributes for mining of association rules,\n",
    "you can use them to definition and solving of more data mining tasks.\n",
    "<br />\n",
    "The following lines demonstrate the definition and execution of another task.\n",
    "It is possible to say that it is only modification of the previous sections 4. - 6.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pattern of association rules you are interested in\n",
    "antecedent = [\n",
    "    {\"attribute\" : attributes_columns_map[\"sepallength\"]},\n",
    "    {\"attribute\" : attributes_columns_map[\"petalwidth\"]},\n",
    "    {\"attribute\" : attributes_columns_map[\"sepalwidth\"]},\n",
    "    {\"attribute\" : attributes_columns_map[\"petallength\"]}\n",
    "]\n",
    "consequent = [\n",
    "    {\n",
    "        \"attribute\" : attributes_columns_map[\"class\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define requested interesting measures:\n",
    "# - following definition of requests minimal values of confidence 0.5 and support 0.1;\n",
    "# - with the same structure, you can add also the interest measure \"LIFT\"\n",
    "interest_measures = [\n",
    "    {\n",
    "        \"name\": \"CONF\", # considence\n",
    "        \"value\": 0.3\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"SUPP\", # support\n",
    "        \"value\": 0.01\n",
    "    }\n",
    "]\n",
    "\n",
    "special_interest_measures = [\n",
    "    {\n",
    "        \"name\": \"CBA\" # request rule prunning using rCBA\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define the name of the prepared task (for better identification of results)\n",
    "task_name = \"Test task - classification\"\n",
    "\n",
    "# Define the maximum count of results\n",
    "max_rules_count = 1000\n",
    "\n",
    "# Compose the body of the task definition request\n",
    "json_data = json.dumps({\n",
    "    \"miner\": miner_id,\n",
    "    \"name\": task_name,\n",
    "    \"limitHits\": max_rules_count,\n",
    "    \"IMs\": interest_measures,\n",
    "    \"specialIMs\": special_interest_measures,\n",
    "    \"antecedent\": antecedent,\n",
    "    \"consequent\": consequent\n",
    "})\n",
    "\n",
    "# Send the request for simple task creation\n",
    "r = requests.post(API_URL + \"/tasks/simple?apiKey=\" + API_KEY, headers = {'Content-Type': 'application/json', \"Accept\": \"application/json\"}, data = json_data.encode())\n",
    "\n",
    "# Get the ID of the created task\n",
    "task2_id = r.json()[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Execute the mining task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the request\n",
    "r = requests.get(API_URL + \"/tasks/\" + str(task2_id) + \"/start?apiKey=\" + API_KEY, headers = {'Content-Type': 'application/json', \"Accept\": \"application/json\"})\n",
    "\n",
    "# Wait for result (dependently on the task definition and the size of analyzed data, it can take even a long tame)\n",
    "while True:\n",
    "    time.sleep(1)\n",
    "    # Check the task state\n",
    "    r = requests.get(API_URL + \"/tasks/\" + str(task2_id) + \"/state?apiKey=\" + API_KEY, headers = {'Content-Type': 'application/json', \"Accept\": \"application/json\"})\n",
    "    task_state = r.json()[\"state\"]\n",
    "    print(\"task_state:\" + task_state)\n",
    "    if task_state == \"solved\":\n",
    "        break\n",
    "    if task_state == \"failed\":\n",
    "        print(\"task failed executing\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Export the task results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the export request\n",
    "r = requests.get(API_URL + '/tasks/' + str(task2_id) + '/rules?apiKey=' + API_KEY, headers = {\"Accept\": \"application/json\"})\n",
    "\n",
    "# Parse the response as JSON\n",
    "task2_rules = r.json()\n",
    "\n",
    "# and then work with the results...\n",
    "task2_rules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
